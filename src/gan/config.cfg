[GAN]
device = cpu

ROLLOUT_UPDATE_RATE = 0.8
ROLLOUT_NUM = 16

G_STEPS = 1
D_STEPS = 5
K_STEPS = 3

SEED = 88
BATCH_SIZE = 64
TOTAL_EPOCHS = 200 
GENERATED_NUM = 10000
VOCAB_SIZE = 10
SEQUENCE_LEN = 20

data = 
data_dir = data
glove = data/glove
data_type = eng


[GENERATOR]
grammar = data/eng.grammar.pkl
sample_dir = data
verbose = True
PRE_G_EPOCHS = 120
G_EMB_SIZE = 32
G_HIDDEN_SIZE = 32
G_LR = 1e-3
G_OPTIM = Adam

attention_hidden_dim = 50
decoder_hidden_dim = 256
encoder_hidden_dim =256
node_embed_dim = 256
ptrnet_hidden_dim = 50
rule_embed_dim = 256
word_embed_dim = 128

batch_size = 10
beam_size = 15
clip_grad = 0
decode_max_time_step = 100 
dropout = 0.2
enable_copy = True
encoder = bilstm
frontier_node_type_feed = True
head_nt_constraint = True
max_epoch = 50
max_query_length = None
model = None
no_copy = False
no_frontier_node_type_feed = False
no_head_nt_constraint = False
no_parent_action_feed = False
no_parent_hidden_state_feed = False
no_tree_attention = False
node_num = 0
optimizer = adam
output_dir = outputs
parent_action_feed = True
parent_hidden_state_feed = True
random_seed = 181783
rule_num = 0
save_per_batch = 4000 
source_vocab_size = 0
target_vocab_size = 0
train_patience = 10
tree_attention = True
valid_metric = bleu
valid_per_batch = 4000

# 
# These should probably be controlled at the commandline
# 

# options: [train,evaluate,decode,interactive]
operation = train

# options: [self, dataset, new]
mode = self

type = test_data
saveto = decode_results.bin
input = decode_results.bin
seq2tree_sample_file = model.sample
seq2tree_id_file = test.id.txt
seq2tree_rareword_map = None
seq2seq_decode_file = 
seq2seq_ref_file = 
is_nbest = False


[DISCRIMINATOR]
data = data/multivac
save = checkpoints
expname = test

# model arguments
vocab_size = 0
input_dim = 300
mem_dim = 150
hidden_dim = 50
freeze_embed = True

# training arguments
epochs = 15
batchsize = 25
lr = 0.01
wd = 1e-4
sparse = False
optim = adagrad
